{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"instructions\" style=\"border-radius: 5px; background-color:#f5f5f5;\" >\n",
    "<h1>Instructions</h1>\n",
    "<p>Look for the <b>4 Your Turn</b> sections to complete the code and/or answer questions.<p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 7 - Theory of Regression and Regularization\n",
    "\n",
    "In this notebook we will explore the mathematical basis of linear statistical models. The emphasis is on the ubiquitious problem of **model overfitting** or **model over-parameterizaton**. \n",
    "\n",
    "Overfitting (or over-parameterization) of machine learning models arrises in any case where the number of model parameters exceeds the effective dimensions of the feature set. This is most often the result of linear dependency between the features. However, using too complex a model can lead to similar problems. In the extreme case, imagine a model with as many free parameters as training cases. This model might fit the training data perfectly, but will show unstable and unexpected results when used for other data. In machine learning terminology, we say that such an unstable model does not **generalize**. \n",
    "\n",
    "Many methods have been developed and continue to be developed to deal with over-paramterized or **ill-posed** machine learning models. In particular, in this notebook we will explore three methods for stabalizing over-parameterized models. \n",
    "\n",
    "- Stepwise regression, wherein features are eliminated from an over-parameterized model in a stepwise fashon.\n",
    "- Using a mathematical **regularization** technique, known as singular value decomposision, to determine the number of meaningful components for a model. \n",
    "- Using **regularizaton** methods known as ridge, lasso, and elastic-net regression to stabilize over-parameterized models. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Note:** To run the code in this note book you must have installed the following packages:\n",
    " - pandas\n",
    " - numpy\n",
    " - seaborn\n",
    " - scikit-learn\n",
    " - statsmodels\n",
    " - matplotlib\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stepwise Regression\n",
    "\n",
    "In this section we will work through an example of stepwise regression using the [Galton family height data](http://www.randomservices.org/random/data/Galton.html). The goal of the model is to predict the height of adult children given information on the height of their parents. \n",
    "\n",
    "<small>Francis Galton, 2017, \"Galton height data\", https://doi.org/10.7910/DVN/T0HSJ1, Harvard Dataverse, V1</small>\n",
    "\n",
    "### Preparing the data\n",
    "\n",
    "As a first step we will create a data set for just the adult male children. The code in the cell below performs the following operations:\n",
    "\n",
    "- Subset the data to just male adult children.\n",
    "- Compute two new feaures, the heights of the parents squared.\n",
    "- Zscore scale the features.\n",
    "\n",
    "Execute this code to prepare the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as sm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "data_file = 'https://library.startlearninglabs.uw.edu/DATASCI410/Datasets/galton.txt'\n",
    "family_data = pd.read_csv(data_file, delimiter='\\t')\n",
    "\n",
    "# Label the columns\n",
    "family_data.columns = [\"family\",\"father\",\"mother\",\"gender\",\"childHeight\", \"kids\"]\n",
    "\n",
    "# Preview the data\n",
    "family_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types for numeric data types\n",
    "family_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset the data with a Boolean Flag\n",
    "isMale = family_data.loc[:,\"gender\"] == \"M\"\n",
    "\n",
    "# Create just the male dataframe\n",
    "male_only = family_data[isMale].copy()\n",
    "\n",
    "# Preview the new dataframe\n",
    "male_only.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display counts of the data frames\n",
    "print('Number of rows: {}, Number of Males: {}'.format(len(family_data), len(male_only)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new data frame for new feature set\n",
    "male_df = male_only.copy()\n",
    "\n",
    "# Add in squares of mother and father heights\n",
    "male_df['father_sqr'] = male_df['father'] **2\n",
    "male_df['mother_sqr'] = male_df['mother'] **2\n",
    "\n",
    "# Drop columns for family, gender, kids\n",
    "Obsolete = [\"family\", \"gender\", \"kids\"]\n",
    "for x in Obsolete:\n",
    "    male_df = male_df.drop(x, axis=1)\n",
    "    \n",
    "# Reset the index\n",
    "male_df=male_df.reset_index(drop=True)\n",
    "\n",
    "# preview the data\n",
    "male_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale everything but the individual height (child height)\n",
    "# Create a scale function for a column in a pandas df\n",
    "def scale(col):\n",
    "    mean_col = np.mean(col)\n",
    "    sd_col = np.std(col)\n",
    "    std = (col - mean_col) / sd_col\n",
    "    return std\n",
    "\n",
    "# Add scaled x to data frame\n",
    "male_df['father'] = scale(male_df['father'])\n",
    "male_df['mother'] = scale(male_df['mother'])\n",
    "male_df['father_sqr'] = scale(male_df['father_sqr'])\n",
    "male_df['mother_sqr'] = scale(male_df['mother_sqr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Computing a model with all features\n",
    "\n",
    "As a first step, let's compute a model for the hight of the adult male childern using all available features. Execute the code in the cell below to compute this model, and print and plot evaluation information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_model = sm.ols(formula = 'childHeight ~ father + mother + father_sqr + mother_sqr + 1', data=male_df)\n",
    "\n",
    "results = ols_model.fit()\n",
    "n_points = male_df.shape[0]\n",
    "y_output = male_df['childHeight'].reshape(n_points, 1)\n",
    "\n",
    "# Get slope (m) and y-intercept (b)\n",
    "print('Intercept, Slopes : \\n{}'.format(results.params))\n",
    "\n",
    "# Get the t-values (hypothesis test statistics) for linear regression coefficient hypothesis tests.\n",
    "print('Intercept t-value, Slope t-values: \\n{}'.format(results.tvalues))\n",
    "\n",
    "# Get p-values for above t-value statistics\n",
    "print('\\nHypothesis test summary for each coefficient if they differ from zero:')\n",
    "print(results.pvalues)\n",
    "\n",
    "print('\\nSSE, SST, SSR, and RMSE:')\n",
    "mean_y = np.mean(y_output)\n",
    "sst = np.sum((y_output - mean_y)**2)\n",
    "sse = sst - results.ssr\n",
    "print('SSE: {}'.format(sse))\n",
    "print('SST: {}'.format(sst))\n",
    "print('SSR: {}'.format(results.ssr))\n",
    "print('RMSE: {}'.format(np.sqrt(results.mse_model)))\n",
    "\n",
    "# Get most of the linear regression statistics we are interested in:\n",
    "print(results.summary())\n",
    "\n",
    "# Plot a histogram of the residuals\n",
    "sns.distplot(results.resid, hist=True)\n",
    "plt.xlabel('Residual')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Residual Histogram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear from the summary that this model is over-parameterized. Only the intercept is significant. In other words, we are computing the average value of the lable (childHeight), but nothing more. Examination of the residual plots shows them to be mostly well behaved, except with a bit of curvature in the standardized residual plot. \n",
    "\n",
    "### Apply stepwise regression\n",
    "\n",
    "Stepwise regression using model performance metrics to prune the number of features in a model. The steps can be forward, wherein features are added one at a time in order of importance, until a point of diminished return is reached. Or, the steps can be backward, wherein a model using all features is pruned one feature at a time in reverse order of importance. It is also possible to step in both directions. In practice, either backward steps or using both directions are used, since forward steps have a tendency to get stuck at poor solutions. \n",
    "\n",
    "A significant issue with stepwise regression is to choose a performance metric. Many commonly used error metrics like RMSE will natually get better as we add more model parameters. Consequently the **Akaike information criterion** (AIC) is often used. We can write the AIC as:\n",
    "\n",
    "$$AIC = 2 k - 2 ln(\\hat{L})\\\\\n",
    "where\\\\\n",
    "\\hat{L} = the\\ likelihood\\ given\\ the\\ fitted\\ model\\ parmaters\\ \\hat\\theta = p(x| \\hat\\theta)\\\\\n",
    "x = observed\\ data\\\\\n",
    "k = number\\ of\\ model\\ parameters$$\n",
    "\n",
    "In words, the AIC is the model log-likelihood adjusted for the number of model parameters. The objective is to minimize the AIC. \n",
    "\n",
    "The quantity $- 2 ln(p(x| \\hat\\theta))$ is sometimes referred to as the **deviance** of the model. Deviance is a measure of the relative likelihood of the model. Deviance is a generalization of the variance. In fact, deviance should be meaured with respect to a saturated model (number of parameters = number of observations), but this step is often skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_selected(data, response):\n",
    "    \"\"\"Linear model designed by forward selection. Based on AIC\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pandas DataFrame with all possible predictors and response\n",
    "\n",
    "    response: string, name of response column in data\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    model: an \"optimal\" fitted statsmodels linear model\n",
    "           with an intercept\n",
    "           selected by forward selection\n",
    "    \"\"\"\n",
    "    # Start with no factors (intercept only)\n",
    "    formula = \"{} ~ 1\".format(response)\n",
    "    best_aic = sm.ols(formula, data).fit().aic\n",
    "    \n",
    "    # Go through remaining sets of variables one-by-one\n",
    "    remaining = set(data.columns)\n",
    "    remaining.remove(response)\n",
    "    selected = []\n",
    "    current_aic = best_aic\n",
    "    \n",
    "    # Check if any variables remain and if we haven't improved by adding any yet\n",
    "    while remaining and current_aic == best_aic:\n",
    "        aic_candidates = []\n",
    "        for candidate in remaining:\n",
    "            # Try adding the candidate column\n",
    "            formula = \"{} ~ {} + 1\".format(response, ' + '.join(selected + [candidate]))\n",
    "            # Get AIC\n",
    "            aic = sm.ols(formula, data).fit().aic\n",
    "            # Append tuple of the form (aic, response)\n",
    "            aic_candidates.append((aic, candidate))\n",
    "        # Sort all the pairs by the first entry of tuple (default of sort() method)\n",
    "        aic_candidates.sort()\n",
    "        # Remember that the sort() method sorts by smallest to largest of first entry here.\n",
    "        #   If you were to change the criteria to something that needs to be maximized, change sort/pop order!\n",
    "        best_new_aic, best_candidate = aic_candidates.pop(0)\n",
    "        # Now check if we have something better:\n",
    "        if best_new_aic < current_aic:\n",
    "            remaining.remove(best_candidate)\n",
    "            selected.append(best_candidate)\n",
    "            current_aic = best_new_aic\n",
    "        # Now we repeat the process with all the remaining candidate columns\n",
    "\n",
    "    # Here is the final formula!\n",
    "    formula = \"{} ~ {} + 1\".format(response, ' + '.join(selected))\n",
    "    # Get the model object\n",
    "    model = sm.ols(formula, data).fit()\n",
    "    return model\n",
    "\n",
    "model = forward_selected(male_df, 'childHeight')\n",
    "\n",
    "print(model.model.formula)\n",
    "\n",
    "print('Adjusted-R-Squared: {0:.3f}'.format(model.rsquared_adj))\n",
    "print('AIC: {0:.3f}'.format(model.aic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Turn 1\n",
    "\n",
    "## Backward stepwise selection\n",
    "\n",
    "Backward stepwise selection is a very similar algorithm.  Fill in the blanks in the following formula for backward stepwise selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_selected(data, response):\n",
    "    \"\"\"Linear model designed by backward selection. Based on AIC\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pandas DataFrame with all possible predictors and response\n",
    "\n",
    "    response: string, name of response column in data\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    model: an \"optimal\" fitted statsmodels linear model\n",
    "           with an intercept\n",
    "           selected by backward selection\n",
    "    \"\"\"\n",
    "    # Start with all factors and intercept\n",
    "    possible_factors = set(data.columns)\n",
    "    possible_factors.remove(response)\n",
    "    formula = \"{} ~ {} + 1\".format(response, ' + '.join(possible_factors))\n",
    "    \n",
    "    # Fill out this formula!\n",
    "    #--------------------------------\n",
    "    # Fill in spot #1!!!!\n",
    "    best_aic = sm.ols(???).fit().aic                  # fill in spot 1, replace ???\n",
    "    #--------------------------------\n",
    "    \n",
    "    current_aic = best_aic\n",
    "    \n",
    "    # Set a non-empty set of columns that will be labeled as \"to remove and try\"\n",
    "    to_try_remove = possible_factors\n",
    "    \n",
    "    # Check if any variables remain and if we haven't improved by adding any yet\n",
    "    while to_try_remove and current_aic == best_aic:\n",
    "        aic_candidates = []\n",
    "        for candidate in to_try_remove:\n",
    "            \n",
    "            columns = possible_factors - set([candidate])\n",
    "            # Try removing the candidate column\n",
    "            formula = \"{} ~ {} + 1\".format(response, ' + '.join(columns))\n",
    "            # Get AIC\n",
    "            aic = sm.ols(formula, data).fit().aic\n",
    "            \n",
    "            # Append tuple of the form (aic, response)\n",
    "            aic_candidates.append((aic, candidate))\n",
    "            \n",
    "        # Sort all the pairs by the first entry of tuple (default of sort() method)\n",
    "        aic_candidates.sort()\n",
    "        # Remember that the sort() method sorts by smallest to largest of first entry here.\n",
    "        #   If you were to change the criteria to something that needs to be maximized, change sort/pop order!\n",
    "        best_new_aic, best_candidate = aic_candidates.pop(0)\n",
    "        \n",
    "        # Now check if we have something better:\n",
    "        if best_new_aic < current_aic:\n",
    "            # Remove the best candidate's name from possible_factors\n",
    "            \n",
    "            #--------------------------------\n",
    "            # Fill in spot #2 & #3!!!!\n",
    "            possible_factors.remove(???)          #fill in spot 2, replace ???\n",
    "            current_aic = ???                     #fill in spot 3, replace ???\n",
    "            #--------------------------------\n",
    "            \n",
    "        # Now we repeat the process with all the remaining candidate columns\n",
    "\n",
    "    # Here is the final formula!\n",
    "    formula = \"{} ~ {} + 1\".format(response, ' + '.join(possible_factors))\n",
    "    # Get the model object\n",
    "    model = sm.ols(formula, data).fit()\n",
    "    return model\n",
    "\n",
    "backwards_model = backward_selected(male_df, 'childHeight')\n",
    "\n",
    "print(backwards_model.model.formula)\n",
    "\n",
    "print('Adjusted R-Squared: {}'.format(backwards_model.rsquared_adj))\n",
    "print('AIC: {}'.format(backwards_model.aic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Model Statistics\n",
    "Now that we have a backwards-selected formula : `childHeight ~ father + mother + mother_sqr + 1`, let's use that formula to get the statistics of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear model stats\n",
    "ols_model_forward = sm.ols(formula = 'childHeight ~ father + mother + mother_sqr + 1', data=male_df)\n",
    "\n",
    "results = ols_model_forward.fit()\n",
    "n_points = male_df.shape[0]\n",
    "y_output = male_df['childHeight'].reshape(n_points, 1)\n",
    "\n",
    "# Get slope (m) and y-intercept (b)\n",
    "print('Intercept, Slopes : \\n{}'.format(results.params))\n",
    "\n",
    "# Get the t-values (hypothesis test statistics) for linear regression coefficient hypothesis tests.\n",
    "print('Intercept t-value, Slope t-values: \\n{}'.format(results.tvalues))\n",
    "\n",
    "# Get p-values for above t-value statistics\n",
    "print('\\nHypothesis test summary for each coefficient if they differ from zero:')\n",
    "print(results.pvalues)\n",
    "\n",
    "print('\\nSSE, SST, SSR, and RMSE:')\n",
    "mean_y = np.mean(y_output)\n",
    "sst = np.sum((y_output - mean_y)**2)\n",
    "sse = sst - results.ssr\n",
    "print('SSE: {}'.format(sse))\n",
    "print('SST: {}'.format(sst))\n",
    "print('SSR: {}'.format(results.ssr))\n",
    "print('RMSE: {}'.format(np.sqrt(results.mse_model)))\n",
    "\n",
    "# Get most of the linear regression statistics we are interested in:\n",
    "print(results.summary())\n",
    "\n",
    "# Plot a histogram of the residuals\n",
    "sns.distplot(results.resid, hist=True)\n",
    "plt.xlabel('Residual')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Residual Histogram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding an interaction term\n",
    "\n",
    "We will try one last idea, adding an interaction term. In this case we will compute all possbile interactions between the heights of the mother and the father, `mother`, `father`, and the interaction term: `mother X father`. \n",
    "\n",
    "Execute the code in the cell below to compute the model and print and plot the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear model stats\n",
    "ols_model_forward = sm.ols(formula = 'childHeight ~ father + mother + mother*father + 1', data=male_df)\n",
    "\n",
    "results = ols_model_forward.fit()\n",
    "n_points = male_df.shape[0]\n",
    "y_output = male_df['childHeight'].reshape(n_points, 1)\n",
    "\n",
    "# Get slope (m) and y-intercept (b)\n",
    "print('Intercept, Slopes : \\n{}'.format(results.params))\n",
    "\n",
    "# Get the t-values (hypothesis test statistics) for linear regression coefficient hypothesis tests.\n",
    "print('Intercept t-value, Slope t-values: \\n{}'.format(results.tvalues))\n",
    "\n",
    "# Get p-values for above t-value statistics\n",
    "print('\\nHypothesis test summary for each coefficient if they differ from zero:')\n",
    "print(results.pvalues)\n",
    "\n",
    "print('\\nSSE, SST, SSR, and RMSE:')\n",
    "mean_y = np.mean(y_output)\n",
    "sst = np.sum((y_output - mean_y)**2)\n",
    "sse = sst - results.ssr\n",
    "print('SSE: {}'.format(sse))\n",
    "print('SST: {}'.format(sst))\n",
    "print('SSR: {}'.format(results.ssr))\n",
    "print('RMSE: {}'.format(np.sqrt(results.mse_model)))\n",
    "\n",
    "# Get most of the linear regression statistics we are interested in:\n",
    "print(results.summary())\n",
    "\n",
    "# Plot a histogram of the residuals\n",
    "sns.distplot(results.resid, hist=True)\n",
    "plt.xlabel('Residual')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Residual Histogram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "It is clear from the summary that the interaction term is not significant. The best model we have to this point is the one computed with stepwise regression. \n",
    "\n",
    "***\n",
    "**Note:** Stepwise regression appears to be a simple method for feature selection. However, be aware that **stepwise regresson does not scale well**. As with any multiple comparison method, stepwise regresson suffers from a high probability of false positive results. In this case, a feature which should be dropped might not be, because of a low p-value or AIC. \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization: Singular Value Decomposition\n",
    "\n",
    "Now that we have explored both manual feature selection and stepwise regression, we will examine regularization methods. Regularization methods stabilize the inverse of the **model matrix**. In this section we will use the singular value decomposition method to stabilize a model matrix. \n",
    "\n",
    "You may well wonder why we need regularization methods, when we have tools like stepwise regression. Two important reasonse are:\n",
    "\n",
    "- Stepwise regression is a compuationally intensive process, since we must recompute the model many times. There are methods that allow computation of the updated model, but with a large number of features there are numerous permutations. We need methods that can handle hundreds, thousands, or even millions of features.\n",
    "\n",
    "> Note: Consider a small data set with only 20 features.  The amount of possible linear models with NO interaction terms is given by:  $\\binom{20}{1} + \\binom{20}{2} + \\binom{20}{3} + \\dots + \\binom{20}{20}$, which is the sum of the 21st row of the Pascal's triangle.  This comes out to be $2^{20} = 1,048,576$.  Although, according to our algorithm, this would be the maximum number of models to compute, you can see how computationally hard this becomes.\n",
    "\n",
    "- With stepwise regression a feature is either in or out of the model. This may not be the best choise. Perhaps a re-weighing the features in some way might be better. \n",
    "- Stepwise regression suffers from issues inherent in multiple comparisons. \n",
    "\n",
    "In order to understand the motivation and methods of feature selection/transformation, we will start with reviewing some linear algebra.\n",
    "\n",
    "### Linear Algebra Review\n",
    "\n",
    "Before we get into the details of regularizaton, let's review some basic linear algebra.\n",
    "\n",
    "For this part of the notebook we will be using the library `numpy`, which is the numerical python library.\n",
    "\n",
    "> Note: In python there are a few distinctions to make.  The base python has a variable type called 'Lists'. Lists are just that. They are lists of objects. `[5, 'foobar', True, ...]`. While we can make a list that contains all numbers: `[2.0, 2.0, 2.0]`, this is not a vector nor matrix.  The package `numpy` introduces arrays (vectors and matrices) which have the correct associated matrix properties.\n",
    "\n",
    "> Note: Please be careful using Jupyter Notebook. Jupyter displays arrays the same as lists (you'll see in examples below).  It is even the case that some Numpy functions and objects will interact with lists the same way, but sometimes they do not and it can be frustrating to troubleshoot if they appear the same.  Always read documentation and check the types of your variables with the command `type(my_variable)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by creating two vectors of length 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_list = [2]*3\n",
    "print(a_list)\n",
    "print(type(a_list))\n",
    "a = np.array([2]*3)\n",
    "print(a)\n",
    "print(type(a))\n",
    "b = np.arange(1, 4)\n",
    "print(b)\n",
    "print(type(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Element-wise Operations\n",
    "We can perform some basic element-wise arithmetic opertions on vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(a + b)\n",
    "print(a * b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These vectors are both dimension (size) three.\n",
    "\n",
    "We can do the same in two - dimensional matrices as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a2d = np.array([[2]*3, [3]*3])\n",
    "print('a: \\n{} \\n a-size: {}'.format(a2d, a2d.shape))\n",
    "b2d = np.reshape(np.arange(1,7), newshape=(2, 3))\n",
    "print('\\nb: \\n{} \\n b-size: {}'.format(b2d, b2d.shape))\n",
    "\n",
    "# Addition\n",
    "print('\\nAddition: \\n {}'.format(a2d + b2d))\n",
    "# Multiplication\n",
    "print('\\nMultiplication: \\n {}'.format(a2d * b2d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transposing a Matrix\n",
    "We can also _transpose_ a two-dimensional matrix by flipping the rows and columns. We use the `numpy` method `np.transpose`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Try transposing on a vector...')\n",
    "print(np.transpose(a))\n",
    "# Uh oh!  Transpose on a vector doesn't do anything...\n",
    "\n",
    "print('Now reshape a into 2D and then transpose:')\n",
    "print(np.transpose(np.reshape(a, newshape=(1,3))))\n",
    "print('Now transpose a 2d matrix:')\n",
    "print(np.transpose(a2d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Dot Product\n",
    "We can also compute the **dot product**, which is also known as the **scalar product** or **inner product** of two vectors of equal length.\n",
    "\n",
    "$$dot\\ product = \\Sigma_i^n a_i \\cdot b_i$$\n",
    "\n",
    "Give the dot product a try by executing the code in the cell below. We use the `numpy` method `np.dot`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('np.dot(a, b) = np.dot({}, {})'.format(a, b))\n",
    "np.dot(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The square root of the inner product of a vector with itself is the length or $L2$ norm of the vector.\n",
    "\n",
    "$$\\parallel a \\parallel = length\\ of\\ vector\\ a = \\sqrt{a \\cdot a}$$\n",
    "\n",
    "--------------------\n",
    "\n",
    "## Your Turn 2\n",
    "\n",
    "Create and exectue the code to compute the length or norm of the vector `a` in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inner_prod(vec1, vec2):\n",
    "    # Replace the ??? in the following calculation:\n",
    "    inner_prod_calc = ???\n",
    "    return inner_prod_calc\n",
    "\n",
    "def l2_norm(my_vec):\n",
    "    # Replace the ??? in the following calculation:\n",
    "    l2_norm_calc = ???\n",
    "    return l2_norm_calc\n",
    "\n",
    "print('L2 Norm of {} = {}'.format(a, l2_norm(a)))\n",
    "# Should get ~3.4641016151377544"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also write the inner product as:\n",
    "\n",
    "$$a \\cdot b = \\parallel a \\parallel \\parallel b \\parallel cos(\\theta)\\\\\n",
    "or \\\\\n",
    "cos(\\theta) = \\frac{a \\cdot b}{\\parallel a \\parallel \\parallel b \\parallel}$$\n",
    "\n",
    "Notice that the inner product of orthogonal vectors is $0$. Run the code in the cell below to see an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = np.array([1, 0, 0])\n",
    "bb = np.array([0, 1, 1])\n",
    "print(inner_prod(aa, bb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try some operations on martricies. Let $A$  and $B$ be $m = 4$ rows by $n = 3$ columns matrices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[4]*3]*4) # A has 4 rows and 3 columns\n",
    "print(A)\n",
    "B = np.array(np.reshape(np.arange(1, 13), newshape = (4, 3)))\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can perform some arithmatic operations element by element on these matrces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(A + B)\n",
    "print(A * B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can multiply a $mxn$ matrix by a vector of length $n$ by taking the inner product of each row of the matrix and the vector. The result in a vector of length $n$. Each element of the result can be written at:\n",
    "\n",
    "$$y_i = \\Sigma_j^m A_{ij} \\cdot b_j$$\n",
    "\n",
    "Run the code in the cell below and examine the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(A, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, how do we multipy two matrices? In matrix multiplcation each element of the resulting matrix is the inner product of a row by a column. For example, the element $Y_{ij}$ of the result matrix is computed as follows:\n",
    "\n",
    "$$Y_{ij} = \\Sigma_j^m A_{ij} \\cdot B_{ji}$$\n",
    "\n",
    "Notice that the number of columns, $m$, of the first matrix must equal the number of rows of second matrix. And, that the number of rows, $n$ of the first matrix must equal the number of columns of the second matrix. In this case, the two matrices are said to be **conformable**. \n",
    "\n",
    "Give matrix multiplication a try by exectuing the code in the cell bellow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That operation failed! Evidently these matrices are not conformable. \n",
    "\n",
    "But, what if we take the transpose of $B$? The **transpose** of a matrix is just that matrix with the row and column indicies permuted like this:\n",
    "\n",
    "$$B_{ji}^T = B_{ij}\\\\\n",
    "where \\\\\n",
    "B\\ has\\ dimensions\\ n x m \\\\\n",
    "and \\\\\n",
    "B^T\\ has\\ dimensions\\ m x n$$ \n",
    "\n",
    "If we multiply an $n x m$ matix by an $m x n$ matrix the result is a square $n x n$ matrix. \n",
    "\n",
    "We can execute the code in the cell below to multiply the matrix A by the transpose of B. Use the np.transpose() function to take the transpose of the matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AtB = np.dot(np.transpose(A), B)\n",
    "print('A\\'*B = \\n{}'.format(AtB))\n",
    "print('Shape of A\\'*B = {}'.format(AtB.shape))\n",
    "\n",
    "ABt = np.dot(A, np.transpose(B))\n",
    "print('\\nA*B\\' = \\n{}'.format(ABt))\n",
    "print('Shape of A*B\\' = {}'.format(ABt.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identity Matrix\n",
    "We can define the **identity** matrix having ones on the diagonal and zeros elsewehere.\n",
    "\n",
    "$$I = \\begin{bmatrix}\n",
    "    1  & 0 & 0 & \\dots & 0 \\\\\n",
    "    0  & 1 & 0 & \\dots & 0 \\\\\n",
    "    \\vdots &\\vdots &\\vdots & & \\vdots \\\\\n",
    "    0 & 0 & 0 & \\dots & 1\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "The identity multiplied by any matrix gives that matrix. If $AB$ is a rectangular matrix then:\n",
    "\n",
    "$$AB = I \\cdot AB = AB \\cdot I$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `numpy` the Identity matrix is called `np.eye` and takes as an argument the number of rows/columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I3 = np.eye(3)\n",
    "I4 = np.eye(4)\n",
    "\n",
    "print('I (3X3) = \\n{}'.format(I3))\n",
    "\n",
    "print(np.dot(I3, AtB))\n",
    "print(np.dot(I4, ABt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverse of a Matrix\n",
    "In principle we can compute an inverse of a matrix so that:\n",
    "\n",
    "$$A = A\\\\\n",
    "A = AI \\\\\n",
    "A^{-1}A = I$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the Linear Algebra methods in `numpy` to get the inverse of matrix $M$ with `np.linalg.inv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = np.array([[1., 4.], [-3., 2.]])\n",
    "M_inverse = np.linalg.inv(M)\n",
    "\n",
    "print('M: \\n{}'.format(M))\n",
    "\n",
    "print('\\nM_inv = \\n{}'.format(M_inverse))\n",
    "\n",
    "print('\\nM_inv * M = \\n{}'.format(np.dot(M_inverse, M)))\n",
    "\n",
    "print('\\nM * M_inv = \\n{}'.format(np.dot(M, M_inverse)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Singular Value Decomposition\n",
    "In machine learning, we often encounter matricies which cannot be inverted directrly. Instead, we need a decompositon of $A$ that allows us to compute $A^{-1}$. One possibility is a method called singular value decomposition or SVD:\n",
    "\n",
    "$$svd(A) = U D V^{\\ast}$$\n",
    "\n",
    "Where,\n",
    "\n",
    "- $U$ are the orthogonal unit norm left singular vectors.\n",
    "- $V$ are the orthogonal unit norm right singular vectors, and $V^{\\ast}$ is the conjugate transpose. For real-valued $A$ this is just $V^T$.\n",
    "- $D$ is a diagonal matrix of singular values, which are said to define a **spectrum**.\n",
    "- $A$ is comprised of the linear combination of singular vectors scaled by singular values.\n",
    "\n",
    "To compute the SVD of a matrix and view the results execute the code in the cell below. We use the `numpy` method called `np.linalg.svd` using `s` for the singular values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, s, V = np.linalg.svd(B, full_matrices=False)\n",
    "print('U: {}'.format(U))\n",
    "print('s: {}'.format(s))\n",
    "print('V: {}'.format(V))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's called a factorization because...\n",
    "# Create a diagonal matrix\n",
    "S = np.diag(s)\n",
    "\n",
    "# Matrix multiply:  U * S * V\n",
    "B_reconstruction = np.dot(U, np.dot(S, V))\n",
    "np.allclose(B, B_reconstruction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Hint: check the values of S, B, and B_reconstruction\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can verify that the singular vectors form a orthonomal basis by checking to see if $U \\cdot U^{T} = U^{T} \\cdot U = I$ and $V \\cdot V^{T} = V^{T} \\cdot V = I$. We can check this by executing the code in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basis_vec_check1 = np.dot(np.transpose(U), U)\n",
    "print(np.round(basis_vec_check1, 2))\n",
    "\n",
    "basis_vec_check2 = np.dot(V, np.transpose(V))\n",
    "print(np.round(basis_vec_check2, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result are two identity matricies!\n",
    "\n",
    "We can view the product of the matrix $A$ with a vector as defining a rotation and scaling. The singular value decomposition of $A$ can be viewed as:\n",
    "\n",
    "- A first rotation defined by the unit norm singular values $V^{\\ast}$.\n",
    "- A scaling defined by the diagonal singular value matrix $D$.\n",
    "- A second rotation defined by the unit norm singular values $U$.\n",
    "\n",
    "This geometric interpertation can be visualized as shown in the figure below.\n",
    "\n",
    "<img src=\"https://library.startlearninglabs.uw.edu/DATASCI410/img/SVD.png\" title=\"Singular Value Decomposition\" style=\"height: 300px;\">\n",
    "\n",
    "Execute the code in the cell below and examine the rotations and scaling of the inital vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-Dimensional Example\n",
    "D = np.array([[3., 4.], [1., 5.]])\n",
    "U, s, V = np.linalg.svd(D, full_matrices=False)\n",
    "e = np.sqrt(1./2.)\n",
    "u = np.array([e, e])\n",
    "\n",
    "print('u: {}'.format(u))\n",
    "print('Magnitude: {}'.format(np.sqrt(np.dot(np.transpose(u), u))))\n",
    "print('\\n')\n",
    "\n",
    "print('First Rotation:')\n",
    "u_rot1 = np.dot(np.transpose(V), u)\n",
    "print(u_rot1)\n",
    "print('Magnitude: {}'.format(np.sqrt(np.dot(np.transpose(u_rot1), u_rot1))))\n",
    "print('\\n')\n",
    "\n",
    "print('Scaling:')\n",
    "u_scaled = np.dot(np.diag(s), u_rot1)\n",
    "print(u_scaled)\n",
    "print('Magnitude: {}'.format(np.sqrt(np.dot(np.transpose(u_scaled), u_scaled))))\n",
    "print('\\n')\n",
    "\n",
    "print('Second Rotation:')\n",
    "u_rot2 = np.dot(U, u_scaled)\n",
    "print(u_rot2)\n",
    "print('Magnitude: {}'.format(np.sqrt(np.dot(np.transpose(u_rot2), u_rot2))))\n",
    "\n",
    "# ---------------\n",
    "# Check with just D * u\n",
    "print('\\n')\n",
    "print('Originally, D * u = {}'.format(np.dot(D, u)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Turn 3:\n",
    "Compute the SVD of matrix $B$ and do the following:\n",
    "- Demonstrate that the 4X3 matrix $B$ gives the same rotation and scale by multipling a vector `u = np.array([e, e, e])` where `e = np.sqrt(1./3.)` as multiplying the decomposition, $U \\cdot S \\cdot V' = B$  by `u`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get SVD decomposition\n",
    "U, s, V = np.linalg.svd(B, full_matrices=False)\n",
    "S = np.diag(s)\n",
    "\n",
    "# Setup a 3-D rotational vector.\n",
    "e = np.sqrt(1./3.)\n",
    "u = np.array([e, e, e])\n",
    "\n",
    "# Replace the ???\n",
    "# Calculate Rotation:\n",
    "# u_rot = V' * u\n",
    "u_rot = ???\n",
    "print('\\nRotation: {}'.format(u_rot))\n",
    "\n",
    "# Calculate Scaling:\n",
    "# u_scaled = diag(s) * u_rot\n",
    "u_scaled = ???\n",
    "print('\\nScaled: {}'.format(u_scaled))\n",
    "\n",
    "# Calculate the last rotation:\n",
    "# u_rot2 = U * u_scaled\n",
    "u_rot2 = ???\n",
    "print('\\nRotation 2: {}'.format(u_rot2))\n",
    "\n",
    "#------ Are they the same? Caluclate just B*u --------\n",
    "\n",
    "# Calculate r = B * u:\n",
    "r = ???\n",
    "print('\\nr = B * u = {}'.format(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudo Inverse\n",
    "We will be interested in the inverse of a matrix A to solve linear regression.  If a matrix, $A$ has a SVD decomposition, $U \\cdot S \\cdot V'$, then the inverse, $A^{-1}$ can be calculated as follows:\n",
    "\n",
    "$$ A^{-1} = \\left( U \\cdot S \\cdot V'  \\right)^{-1} $$\n",
    "\n",
    "$$ = V'^{-1} \\cdot S^{-1} \\cdot U^{-1} $$\n",
    "\n",
    "From before, we know (and you can verify with numpy operations) that the inverse of the matrices $V$ and $U$ are transposes. I.e, $U^{-1} = U'$ and $V^{-1}=V'$.  Also, remember that $S$ is a diagonal matrix, with zero values off the diagonal. The inverse of $S$ is easy to calculate:\n",
    "\n",
    "If,\n",
    "\n",
    "$$\n",
    "S = \\textrm{diag}(s_{1}, s_{2}, ...)\n",
    "$$\n",
    "\n",
    "Then,\n",
    "\n",
    "$$\n",
    "S^{-1} = \\textrm{diag}\\left( \\frac{1}{s_{1}}, \\frac{1}{s_{2}} , ... \\right)\n",
    "$$\n",
    "\n",
    "We end up with:\n",
    "\n",
    "$$ A^{-1} = V \\cdot S^{-1} \\cdot U' $$\n",
    "\n",
    "This representation of $A^{-1}$ is called the **pseudo inverse** also known as the **Moore-Penrose inverse**.\n",
    "\n",
    "It is commonly written as:\n",
    "\n",
    "$$A^\\dagger = V \\cdot D^+ \\cdot U'$$\n",
    "\n",
    "Where,\n",
    "\n",
    "- $D^+$ is the square diagonal matrix of **inverse singular values** significantly greater than $0$. All other terms are set to $0$.\n",
    "- $U'$ is the transpose of the right sigular value matrix. \n",
    "- $V$ is the left singular value matrix.\n",
    "\n",
    "The matrix, $A$ may not be of full rank. The types of long and narrow $n x m$ matricies we encounter in machine learning are typically **rank deficient**. A rank deficient matrix arises when there is linear dependency between one or more of the columns. As an example, a matrix with correlated (not necessiarily perfectly correlated) columns is bound to be rank deficient. \n",
    "\n",
    "A matrix is considered rank deficient if it has one or more of the $m$ singular values  $d_i  \\sim 0.0$. In this case we substitute $0.0$ values on the diagonal of $D^+$ where the singular values $d_i \\sim 0.0$. In fact, we want $d_i$ to be significanlty greater than $0$. \n",
    "\n",
    "Let's try an example. The code in the cell below computes the SVD of a matrix of random numbers chosen from a Normal distribution. The pseudo inverse is computed and multiplied by the original matrix. Execute this code and note the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Create a matrix of random normal values:')\n",
    "C = np.array(np.random.randn(3, 3))\n",
    "print(C)\n",
    "\n",
    "print('\\nCompute the SVD and look at the singular values:')\n",
    "U, s, V = np.linalg.svd(C, full_matrices=False)\n",
    "print(s)\n",
    "\n",
    "print('\\nThe inverse matix of singular values:')\n",
    "D = np.diag(np.reciprocal(s))\n",
    "print(D)\n",
    "\n",
    "print('\\nThe Pseudo-inverse of the matrix:')\n",
    "cInv = np.dot(np.dot(np.transpose(V), D), np.transpose(U))\n",
    "print(cInv)\n",
    "\n",
    "print('\\bThe Pseudo-inverse times the matrix:')\n",
    "out = np.dot(cInv, C)\n",
    "print(np.round(out, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, notice that the singular values are of similar magnitude and none are near zero. This matrix is not rank deficient. \n",
    "\n",
    "### Pseudo Inverse Example\n",
    "Let's try another example. The code in the cell below does the following:\n",
    "\n",
    "- Creates a $4 X 4$ matrix of numbers drawn from a Normal distribution.\n",
    "- Substitues values in the 4th column which are a linear combination of the other three columns.\n",
    "- Computes the SVD of this matrix.\n",
    "- Creates the inverse diagonal matrix of singular values.\n",
    "- Computes the pseudo inverse of the matrix.\n",
    "- Multiplies the pseudo inverse by the original matrix. \n",
    "\n",
    "Execute this code and examine the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Create a matrix of random normal values:')\n",
    "C = np.array(np.random.randn(4, 4))\n",
    "# Rewrite the 4th column as a linear combination of the prior 3 columns\n",
    "C[:, 3] = 0.4 * C[:, 0] + 0.2 * C[:, 1] + 0.4 * C[:, 2]\n",
    "print(C)\n",
    "\n",
    "print('\\nCompute the SVD and look at the singular values:')\n",
    "U, s, V = np.linalg.svd(C, full_matrices=False)\n",
    "print(s)\n",
    "\n",
    "print('\\nThe inverse matix of singular values:')\n",
    "D = np.diag(np.reciprocal(s))\n",
    "print(D)\n",
    "\n",
    "print('\\nThe Pseudo-inverse of the matrix:')\n",
    "cInv = np.dot(np.dot(np.transpose(V), D), np.transpose(U))\n",
    "print(cInv)\n",
    "\n",
    "print('\\bThe Pseudo-inverse times the matrix:')\n",
    "out = np.dot(cInv, C)\n",
    "print(np.round(out, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the following about this result:\n",
    "\n",
    "- The 4th singular value is nearly zero. Evidently, this matrix is rank deficient. \n",
    "- The product of the pseudo inverse is not close to being the identity matrix. This is the result of taking using the unstable inverse of the rank deficient matrix.\n",
    "\n",
    "Instead, let's set the singular value that is near zero to actually zero.\n",
    "\n",
    "- Set the inverse of the smallest singular value to zero.\n",
    "- Compute the pseudo inverse.\n",
    "- Compute the product of the pseudo inverse and the orginal matrix. \n",
    "- Note if the result is closer to an identity matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The inverse matrix of singular values:')\n",
    "print(D)\n",
    "\n",
    "# Fill in the missing spots\n",
    "print('\\nSet the near-zero singular values to zero:')\n",
    "D[3, 3] = 0.0\n",
    "print(D)\n",
    "\n",
    "print('\\nThe pseudo-inverse of the matrix:')\n",
    "cInv = np.dot(np.dot(np.transpose(V), D), np.transpose(U))\n",
    "print(cInv)\n",
    "\n",
    "print('\\nThe pseudo-inverse times the matrix:')\n",
    "out = np.dot(cInv, C)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression with the Pseudo Inverse\n",
    "\n",
    "We have already looked at feature selection using manual trial and error methods and stepwise regression. How can we use the pseudo inverse to create a regularized regression? \n",
    "\n",
    "Let's start by examining the linear regression problem. The goal is to compute a vector of **model coefficients** or weights which minimize the mean squared residuals, given a vector of data $x$ amd a **model matrix** $A$. We can write our model as:\n",
    "\n",
    "$$x = A b$$\n",
    "\n",
    "To solve this problem we would ideally like to compute:\n",
    "\n",
    "$$b = A^{-1}x$$\n",
    "\n",
    "However, this is hard to do directly in practice.\n",
    "\n",
    "- In typical case $A$ is long and narrow. In other words we have more data **cases** than coeficients. \n",
    "- Solving for $A^{-1}$ is computationally difficult and inefficient.\n",
    "- Solution is numerically unstable if $A$ is rank deficient. \n",
    "\n",
    "One way to deal with the problem of rank deficiency is to use the pseudo inverse $A^\\dagger$. Recalling the singular value decomposition of $A$ we can write:\n",
    "\n",
    "$$A^\\dagger = V D^+ U^*$$\n",
    "\n",
    "***\n",
    "**Note:** In practice, the direct compuation of a pseudo inverse is rarely used for linear models. Instead, more compuationally efficient methods such as QR decomposition are often used. Discussion of these methods is beyond the scope of this course. Details can be found in many sources including the seminal book titled [Matrix Computations](http://web.mit.edu/ehliu/Public/sclark/Golub%20G.H.,%20Van%20Loan%20C.F.-%20Matrix%20Computations.pdf) by Gene Golub and Charles van Loan.\n",
    "***\n",
    "\n",
    "As a first step we need to create a model matrix of the features for the Galton height data. Execute the code in the cell below which creates a matrix from the features in a data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_df_vars = male_df.loc[:,['mother', 'father','mother_sqr', 'father_sqr']]\n",
    "M = male_df_vars.as_matrix()\n",
    "childHeight = male_df.loc[:,'childHeight']\n",
    "\n",
    "print('1st 5 rows of Data Frame: ')\n",
    "print(male_df_vars.head())\n",
    "print('\\n1st 5 rows in matrix Form: ')\n",
    "print(M[0:5, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to compute the SVD of the resulting matrix and examine the singular values. In the cell below, execute code to do the following:\n",
    "1. We will use Numpy's built in function, `np.linalg.lstsq` to perform linear regression via the SVD.\n",
    "2. We extract the beta coefficients.\n",
    "3. From these coefficients, we perform the predictions.\n",
    "4. We will also compute the R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_coeffs, resids, rank, s = np.linalg.lstsq(M,childHeight)\n",
    "\n",
    "print('beta coefficients: {}'.format(beta_coeffs))\n",
    "\n",
    "print('\\nsingular values: {}'.format(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the 3rd and 4th singular values are a few orders of magnitude smaller than the first two. This matrix is most likely rank deficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "Now that we have a vector of model coefficients, its time to evaluate our model.  We use the following steps:\n",
    "\n",
    "- Compute the predicted values or scores using the product of the model matrix and the model coefficients. We need to add the mean of the label values.\n",
    "- Compute the residuals.\n",
    "- Display residual plots and summary statistics.\n",
    "\n",
    "Execute this code and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "male_predictions = np.dot(M, beta_coeffs) + np.mean(childHeight)\n",
    "male_resids = male_predictions - childHeight\n",
    "\n",
    "# Plot the residuals vs score (height)\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(male_predictions, male_resids, '.')\n",
    "plt.ylabel('residuals')\n",
    "plt.xlabel('score(height)')\n",
    "plt.title('Residuals vs Score')\n",
    "\n",
    "# Histogram of residuals\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(male_resids)\n",
    "plt.title('Histogram of Residuals')\n",
    "\n",
    "# R-squared\n",
    "#              SSR\n",
    "# R^2 = 1 -  -------\n",
    "#              SST\n",
    "\n",
    "SSR = np.sum(np.square(male_resids))\n",
    "SST = np.sum(np.square(childHeight - np.mean(childHeight)))\n",
    "\n",
    "print('SSR: {}'.format(SSR))\n",
    "print('SST: {}'.format(SST))\n",
    "\n",
    "R2 = 1.0 - (SSR / SST)\n",
    "\n",
    "print('R-squared = {}'.format(R2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Regression\n",
    "\n",
    "In various regression transformations we have applied, we haven't yet asked ourselves, \"What is a good transformation to the independent features that may create an optimal amount of independence between them?\"\n",
    "\n",
    "Here, the idea is to combine the features into new wholly independent features by combining them in various linear fashions.\n",
    "\n",
    "Imagine we are performing linear regression on multiple features (n of them), where our model is as follows:\n",
    "\n",
    "$$\n",
    "y_{i} = \\beta_{0} + \\beta_{1}x_{1} + ... + \\beta_{n}x_{n} + \\epsilon_{i}\n",
    "$$\n",
    "\n",
    "We might just procede forward and assume that our x-features are independent and see how our model performs.  What if we could find new features such that they are guarenteed to be completely independent of each other as follows:\n",
    "\n",
    "$$\n",
    "y_{i} = \\beta_{0} + \\beta_{1}\\cdot f_{1} \\left( x_{1}, ... , x_{n} \\right) + ... + \\beta_{n}f_{n} \\left( x_{1}, ... , x_{n} \\right) + \\epsilon_{i}\n",
    "$$\n",
    "\n",
    "Here, we are creating new features, which are linear combinations of all our prior x-features in our $f()$ functions.  If we write out what an $f()$ function might look like:\n",
    "\n",
    "$$\n",
    "f_{m} = a_{0} + a_{1} \\cdot x_{1} + ... + a_{n} \\cdot x_{n}\n",
    "$$\n",
    "\n",
    "The question still remains, how do we find such magical $f()$ functions that assures us that the new features, $f_{m}$ taken together are 100% independent of each other?\n",
    "\n",
    "The method of finding these very important components is called Principal Component Analysis (or PCA).  Then when we use these component functions to do regression, it is called Principal Component Regression (or PCR).\n",
    "\n",
    "It is considered a regularized regression because, typically, we end up with less features than we started with because, in reality, there is always some dependence in our data.\n",
    "\n",
    "You can imagine that, depending on how much dependence is in our data, that the first components found will explain a majority of the variance in the response and the last components add very little explanatory power.  It is common to choose a smaller subset of PCA regressors to do PCR.\n",
    "\n",
    "The benefits of PCR is that it assures us that our new features are completely independent, which overcomes the multicollinearity problem in regression.\n",
    "\n",
    "The downside is that the new features, being functions of variables, are less interpretable than using the original features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------\n",
    "PCR has these steps (commonly):\n",
    "\n",
    "--------------------------\n",
    "\n",
    "0. Scale the input features in your data matrix!\n",
    "\n",
    "1. Perform PCA on the observed data matrix to obtain the principal components\n",
    "\n",
    "2. Select a subset of the principal components for regression. Usually we select the components that explain the most of the variance in the dependent variable.\n",
    "\n",
    "3. Perform linear regression using the selected principal components as features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason we cover this regression method after introducing singular value decomposition is that the SVD is used to find the principal components of the data matrix.\n",
    "\n",
    "For exploring this method, we use generated data to illustrate how components work on features that are not fully independent.\n",
    "\n",
    "To compute PCR, we will use the PCA function from the `sklearn decomposition` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "np.random.seed(10)\n",
    "\n",
    "# We right-multiply a 2x2 matrix to a 2x100 matrix to transform the points to be related.\n",
    "X = np.dot(np.random.randn(100, 2), np.random.rand(2, 2))\n",
    "# Scale the points\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1])\n",
    "plt.grid()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Generated x-y data with dependence')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the first two principal components, because we can visualize 2 dimensions in a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(X)\n",
    "pca_df = pd.DataFrame(data = pca_result , columns = ['pc1', 'pc2'])\n",
    "\n",
    "print(pca_df.head())\n",
    "print(pca_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the output!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(pca_df.loc[:, 'pc1'], pca_df.loc[:, 'pc2'])\n",
    "plt.ylabel('Component 2')\n",
    "plt.xlabel('Component 1')\n",
    "plt.title('Two Dimensional PCA')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at how much variance _in the original features_ we are explaining with each principal component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca.explained_variance_)\n",
    "print(pca.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we interpret the above variances and components?\n",
    "\n",
    "For the \"explained variance\" we interpret those values as the variance of the dataset when all the data is projected onto that $f()$ function axis.  So here, the first $f()$ function (or principal component) explains 1.95 units of the variance. The second component explains the remaining 0.05 units of variance.\n",
    "\n",
    "For the components, this tells us the direction of the principal components. In fact, all principal components (i.e. rows) in this matrix have the property that the magnitude of them is exactly 1.  Here, $\\sqrt{(-0.707...)^{2} + (-0.707...)^{2}} = 1$.\n",
    "\n",
    "The combination of these two matrices (explained variance and component direction) tells us that we can plot these vectors amongst the original data to see them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_var = pca.explained_variance_\n",
    "components = pca.components_\n",
    "\n",
    "# Vector to plot\n",
    "#    = plot scale * component direction * component length\n",
    "v1 = 2 * components[0] * np.sqrt(exp_var[0])\n",
    "v2 = 2 * components[1] * np.sqrt(exp_var[1])\n",
    "c = (0, 0) # Center is at 0,0 because of our standardization\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha=0.5)\n",
    "plt.annotate('', c + v1, c, arrowprops={'arrowstyle': '->', 'shrinkA': 0,\n",
    "                                        'shrinkB': 0, 'linewidth': 3})\n",
    "plt.annotate('', c + v2, c, arrowprops={'arrowstyle': '->', 'shrinkA': 0,\n",
    "                                        'shrinkB': 0, 'linewidth': 3})\n",
    "plt.axis('equal')\n",
    "plt.grid()\n",
    "plt.title('F-vector functions (Principal Components)')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What now?\n",
    "\n",
    "\n",
    "From the above plot, we can see that the two vectors we've plotted are completely independent of each other. In other words, they are perpindicular!\n",
    "\n",
    "We can extend this to higher dimensional data and it has many uses:\n",
    "\n",
    "- If we have very high dimensions, we can reduce the number of dimensions needed for a good regression fit.\n",
    "\n",
    "- Calculating the Principal Components removes any multi-collinearity issues in the data.\n",
    "\n",
    "- We can project the original data points onto our principal component axes and perform regression.\n",
    "\n",
    "We will combine everything we've learned into a principal component linear regression with actual data below. (Galton Height Data for females this time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Turn 4:\n",
    "\n",
    "If our Galton height data set has 4 features, how many principal components will there be?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us compute all the principal components for our Galton height data on the female data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset the data with a Boolean Flag\n",
    "isFemale = family_data.loc[:,\"gender\"] == \"F\"\n",
    "\n",
    "# Create just the female dataframe\n",
    "female_data = family_data[isFemale].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new data frame for new feature set\n",
    "female_df = female_data.copy()\n",
    "\n",
    "# Add in squares of mother and father heights\n",
    "female_df['father_sqr'] = female_df['father'] **2\n",
    "female_df['mother_sqr'] = female_df['mother'] **2\n",
    "\n",
    "# Drop columns for family, gender, kids\n",
    "Obsolete = [\"family\", \"gender\", \"kids\"]\n",
    "for x in Obsolete:\n",
    "    female_df = female_df.drop(x, axis=1)\n",
    "    \n",
    "# Reset the index\n",
    "female_df=female_df.reset_index(drop=True)\n",
    "\n",
    "# Add scaled x to data frame\n",
    "female_df['father'] = scale(female_df['father'])\n",
    "female_df['mother'] = scale(female_df['mother'])\n",
    "female_df['father_sqr'] = scale(female_df['father_sqr'])\n",
    "female_df['mother_sqr'] = scale(female_df['mother_sqr'])\n",
    "\n",
    "# preview the data\n",
    "female_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate all the Principal components\n",
    "X = female_df.as_matrix(columns=['father', 'mother', 'father_sqr', 'mother_sqr'])\n",
    "y = female_df['childHeight']\n",
    "pca = PCA()\n",
    "pca_result = pca.fit_transform(X)\n",
    "pca_df = pd.DataFrame(data = pca_result, columns=['pc1', 'pc2', 'pc3', 'pc4'])\n",
    "\n",
    "# Here is our data projected onto the four principal components.\n",
    "print(pca_df.head())\n",
    "print(pca_df.shape)\n",
    "\n",
    "pca_df['childHeight'] = female_df['childHeight']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all our data projected on our 4 total principal components, let's just look at the explained variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcr_model = sm.ols(formula = 'childHeight ~ pc1 + pc2 + pc3 + pc4', data=pca_df)\n",
    "\n",
    "results = pcr_model.fit()\n",
    "n_points = pca_df.shape[0]\n",
    "y_output = pca_df['childHeight'].values.reshape(n_points, 1)\n",
    "\n",
    "# Get most of the linear regression statistics we are interested in:\n",
    "print(results.summary())\n",
    "\n",
    "# Plot a histogram of the residuals\n",
    "sns.distplot(results.resid, hist=True)\n",
    "plt.xlabel('Residual')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Residual Histogram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, you may see that the last two principal components are not needed for a significant fit.  You could also have noticed that from the 'explained variance' for each component- and how the last two components explained nearly 4 orders of magnitude less variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Regularization with Ridge and Lasso Regression\n",
    "\n",
    "So far, we have looked at two approached for dealing with over-parameterized models; feature selection by stepwise regresson and singular value decomposision. In this section we will explore the most widely used regularization method for optimization-based machine learning models, **ridge regression**. \n",
    "\n",
    "Let's start by examining the matrix-equation formulation of the linear regression problem. The goal is to compute a vector of **model coefficients** or weights which minimize the mean squared residuals, given a vector of data $x$ and a **model matrix** $A$. We can write our model as:\n",
    "\n",
    "$$x = A b$$\n",
    "\n",
    "To solve this problem we would ideally like to compute:\n",
    "\n",
    "$$b = A^{-1}x$$\n",
    "\n",
    "Rewriting this equation,\n",
    "\n",
    "$$b = A^{-1} (A^{T})^{-1} A^{T}x$$\n",
    "\n",
    "Or, alternatively,\n",
    "\n",
    "$$b = (A^TA)^{-1}A^Tx$$\n",
    "\n",
    "Now, $A^TA$ is an $m x m$ matrix, and thus is of reduced dimension. But, **$A^TA$ can still be rank deficient!** \n",
    "\n",
    "The basic idea of ridge regression is to stabilize the inverse singular value matrix, $D^+=A^{T}A$, by **adding a small bias term**, $\\lambda$, to each of the singular values. We can state this operation in matrix notation as follows. We start with a modified form of the normal equations (also know as the **L2 or Euclidean norm** minimization problem):\n",
    "\n",
    "$$min [\\parallel A \\cdot x - b \\parallel + \\parallel \\lambda \\cdot b\\parallel]\\\\  or \\\\\n",
    "b = (A^TA + \\lambda^2)^{-1}A^Tx$$\n",
    "\n",
    "In this way, the values of small singular values do not blow up when we compute the inverse. You can see this by writing out the $D^+$ matrix with the bias term.\n",
    "\n",
    "$$D_{ridge}^+  = \\begin{bmatrix}\n",
    "    \\frac{1}{d_1 + \\lambda^2}  & 0 & 0 & \\dots & 0 \\\\\n",
    "    0  & \\frac{1}{d_2 + \\lambda^2} & 0 & \\dots & 0 \\\\\n",
    "    \\vdots &\\vdots &\\vdots & & \\vdots \\\\\n",
    "    0 & 0 & 0 & \\dots & \\frac{1}{d_m + \\lambda^2}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Adding this bias term creates a 'ridge' in the singular value matrix, giving this method its name **ridge regression**. \n",
    "\n",
    "You can also think of ridge regression as limiting the L2 or Euclidean norm of the values of the model coefficient vector. The value of $\\lambda$ determines how much the norm of the coefficient vector constrains the solution. You can see a view of this geometric interpretaton in the figure below.  \n",
    "\n",
    "<img src=\"https://library.startlearninglabs.uw.edu/DATASCI410/img/L2.jpg\" title=\"Geometric View of L2 regularization\" style=\"height: 300px;\">\n",
    "<center> **Geometric view of L2 regularization**\n",
    "\n",
    "The same method goes by some other names, as it seems to have been 'invented' several times. In particular, **Tikhonov regularization**, or **L2 norm regularization**. In all likelihood the method was first developted by the Russian mathematician Andrey Tikhonov in the late 1940's, and first published in English in 1977.\n",
    "\n",
    "Let's give this a try. Execute the code in the cell below which computes the $(A^TA + \\lambda^2)^{-1}A^T$ matrix with a lambda value of `0.1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, s, V = np.linalg.svd(M, full_matrices=False)\n",
    "\n",
    "# Calculate the inverse singular value matrix from SVD\n",
    "lambda_val = 1.0\n",
    "d = np.diag(1. /  (s + lambda_val))\n",
    "\n",
    "print('Inverse Singular Value Matrix:')\n",
    "print(d)\n",
    "\n",
    "# Compute pseudo-inverse\n",
    "mInv = np.dot(np.transpose(V), np.dot(d, np.transpose(U)))\n",
    "\n",
    "print('M Inverse')\n",
    "print(mInv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we compute the model coeficients using the $(A^TA + \\lambda^2)^{-1}A^T$ matrix we just computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_coeffs_ridge = np.dot(mInv, childHeight)\n",
    "print('Beta\\'s: {}'.format(beta_coeffs_ridge))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias-variance trade-off\n",
    "\n",
    "The statsmodels package allows us to compute a sequence of ridge regression solutions.  The function that does this uses a method called 'elastic-net', know that ridge regression is a specific case of elastic-net, and we will talk more about this later.\n",
    "\n",
    "The code in the cell below computes solutions for `20` values of $\\lambda$. Execute this code and examine the values of the model coefficients as $\\lambda$ increases. We'll use the `numpy` method `np.linspace` to compute sequence of lambdas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge Regression with various penalties in Statsmodels\n",
    "# Generate a sequence of lambdas\n",
    "log_lambda_seq = np.linspace(-6, 2, 50)\n",
    "lambda_seq = np.exp(log_lambda_seq)\n",
    "\n",
    "coeffs_array = []\n",
    "rsq_array = []\n",
    "formula = 'childHeight ~ mother + father + mother_sqr + father_sqr + 1'\n",
    "\n",
    "for lamb in lambda_seq:\n",
    "    ridge_model = sm.ols(formula, data=male_df).fit_regularized(method='elastic_net', alpha=lamb, L1_wt=0)\n",
    "    coeffs_array.append(list(ridge_model.params))\n",
    "    predictions = ridge_model.fittedvalues\n",
    "    residuals = [x - y for x, y in zip(np.squeeze(predictions), childHeight)]\n",
    "\n",
    "    SSR = np.sum(np.square(residuals))\n",
    "    SST = np.sum(np.square(childHeight - np.mean(childHeight)))\n",
    "\n",
    "    rsq = 1 - (SSR / SST)\n",
    "    rsq_array.append(rsq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull out partial slopes (drop intercept version)\n",
    "beta_coeffs = [x[1:] for x in coeffs_array]\n",
    "plt.plot(log_lambda_seq, beta_coeffs)\n",
    "plt.title('Partial Slopes vs Log-Lambda')\n",
    "plt.ylabel('Partial Slope Values')\n",
    "plt.xlabel('Log-Lambda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot partial slopes vs R squared (% deviance explained)\n",
    "plt.plot(rsq_array, beta_coeffs)\n",
    "plt.xlim([0.0, 0.25])\n",
    "plt.title('Partial Slopes vs Log-Lambda')\n",
    "plt.xlabel('R-squared')\n",
    "plt.ylabel('Partial Slopes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Notice that $\\lambda$ increases, the values of the 4 model coefficients decrease toward zero. When all coefficients are zero, the model predicts all values of the label as zero! In other words, high values of $\\lambda$ give highly biased soluions, but with very low variance. For small values of $\\lambda$, the situation is just the opposite. The solution has low bias, but is quite unstable, having maximum variance. This **bias-variance trade off** is a key concept in machine learning.\n",
    "\n",
    "> Also note how we can get a significant increase in R^2 (from 0 --> ~0.20) without changing the partial slopes much.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----------------------------\n",
    "### Lasso regression\n",
    "----------------------------\n",
    "\n",
    "We can also do regularization using other norms. **Lasso or L1 regularizaton** limits the sum of the absolute values of the model coefients. The L1 norm is sometime know as the **Manhattan norm**, since distrance are measured as if you were traveling on a rectangular grid of streets. \n",
    "\n",
    "You can also think of lasso regression as limiting the L1 norm of the values of the model coefficient vector. The value of $\\lambda$ determines how much the norm of the coefficient vector constrains the solution. You can see a view of this geometric interpretaton in the figure below.  \n",
    "\n",
    "<img src=\"https://library.startlearninglabs.uw.edu/DATASCI410/img/L1.jpg\" title=\"Geometric view of L1 regularization\" style=\"height: 300px;\">\n",
    "<center> **Geometric view of L1 regularization**\n",
    "\n",
    "By setting the `alpha` argument to the statsmodels functions to zero you can perform lasso regresson. Execute the code in the cell below to compute and evaluate a lasso regression model with 20 values of lambda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso Regression with a sequence of lambdas\n",
    "# Generate a sequence of lambdas\n",
    "log_lambda_seq = np.linspace(-6, 2, 50)\n",
    "lambda_seq = np.exp(log_lambda_seq)\n",
    "\n",
    "coeffs_array = []\n",
    "rsq_array = []\n",
    "formula = 'childHeight ~ mother + father + mother_sqr + father_sqr + 1'\n",
    "\n",
    "for lamb in lambda_seq:\n",
    "    ridge_model = sm.ols(formula, data=male_df).fit_regularized(method='elastic_net', alpha=lamb, L1_wt=1)\n",
    "    coeffs_array.append(list(ridge_model.params))\n",
    "    predictions = ridge_model.fittedvalues\n",
    "    residuals = [x - y for x, y in zip(np.squeeze(predictions), childHeight)]\n",
    "\n",
    "    SSR = np.sum(np.square(residuals))\n",
    "    SST = np.sum(np.square(childHeight - np.mean(childHeight)))\n",
    "\n",
    "    rsq = 1 - (SSR / SST)\n",
    "    rsq_array.append(rsq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull out partial slopes (drop intercept version)\n",
    "beta_coeffs = [x[1:] for x in coeffs_array]\n",
    "plt.plot(log_lambda_seq, beta_coeffs)\n",
    "plt.title('Partial Slopes vs Log-Lambda')\n",
    "plt.ylabel('Partial Slope Values')\n",
    "plt.xlabel('Log-Lambda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot partial slopes vs R squared (% deviance explained)\n",
    "plt.plot(rsq_array, beta_coeffs)\n",
    "plt.xlim([0.0, 0.25])\n",
    "plt.title('Partial Slopes vs Log-Lambda')\n",
    "plt.xlabel('R-squared')\n",
    "plt.ylabel('Partial Slopes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that model coefficients are much more tightly constrianed than for L2 regularization. In fact, only two of the possible model coefficients have non-zero values at all. This is typical of L1 or lasso regression.\n",
    "\n",
    "---------------\n",
    "### Elastic net regression\n",
    "---------------\n",
    "\n",
    "The **elastic net** algorthm uses a weighted combination of L2 and L1 regularization. As you can probably see, the same function is used for Lasso and Ridge regression with only the `L1_wt` argument changing. This argument determines the how much weight goes to the L1-norm of the partial slopes. If `L1_wt = 0`, the regularization is pure L2 (Ridge) and if `L1_wt = 1.0` the regularization is pure L1 (Lasso).\n",
    "\n",
    "The code in the cell below gives equal weight to each regression method. Excute this code and examine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elasticnet Regression with a sequence of lambdas\n",
    "# Generate a sequence of lambdas\n",
    "log_lambda_seq = np.linspace(-6, 0, 50)\n",
    "lambda_seq = np.exp(log_lambda_seq)\n",
    "\n",
    "coeffs_array = []\n",
    "rsq_array = []\n",
    "formula = 'childHeight ~ mother + father + mother_sqr + father_sqr + 1'\n",
    "\n",
    "for lamb in lambda_seq:\n",
    "    ridge_model = sm.ols(formula, data=male_df).fit_regularized(method='elastic_net', alpha=lamb, L1_wt=0.75)\n",
    "    coeffs_array.append(list(ridge_model.params))\n",
    "    predictions = ridge_model.fittedvalues\n",
    "    residuals = [x - y for x, y in zip(np.squeeze(predictions), childHeight)]\n",
    "\n",
    "    SSR = np.sum(np.square(residuals))\n",
    "    SST = np.sum(np.square(childHeight - np.mean(childHeight)))\n",
    "\n",
    "    rsq = 1 - (SSR / SST)\n",
    "    rsq_array.append(rsq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull out partial slopes (drop intercept version)\n",
    "beta_coeffs = [x[1:] for x in coeffs_array]\n",
    "plt.plot(log_lambda_seq, beta_coeffs)\n",
    "plt.title('Partial Slopes vs Log-Lambda')\n",
    "plt.ylabel('Partial Slope Values')\n",
    "plt.xlabel('Log-Lambda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot partial slopes vs R squared (% deviance explained)\n",
    "plt.plot(rsq_array, beta_coeffs)\n",
    "plt.xlim([0.0, 0.25])\n",
    "plt.title('Partial Slopes vs Log-Lambda')\n",
    "plt.xlabel('R-squared')\n",
    "plt.ylabel('Partial Slopes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the elastic net model combines some of the behaviors of both L2 and L1 regularization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Variables and the Model Matrix\n",
    "\n",
    "Up until now we have only been working with numeric data. How can we handle categorical variables in numeric models? \n",
    "\n",
    "We need to encode the categorical variables into one or more numeric variables. The common approach is to convert the categorical variable to a set of binary **dummy variables** or **indicator variables**. \n",
    "\n",
    "The code in the cell below computes the scaled model data frame. Execute this code and examine the summary of the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full original Family dataset\n",
    "family_data[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new data frame for new feature set\n",
    "height_df = family_data.copy()\n",
    "\n",
    "# Add in squares of mother and father heights\n",
    "height_df['father_sqr'] = height_df['father'] **2\n",
    "height_df['mother_sqr'] = height_df['mother'] **2\n",
    "\n",
    "# Drop columns for family, kids\n",
    "Obsolete = [\"family\", \"kids\"]\n",
    "for x in Obsolete:\n",
    "    height_df = height_df.drop(x, axis=1)\n",
    "    \n",
    "# Reset the index\n",
    "male_df=male_df.reset_index(drop=True)\n",
    "\n",
    "# preview the data\n",
    "height_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add scaled x to data frame\n",
    "height_df['father'] = scale(height_df['father'])\n",
    "height_df['mother'] = scale(height_df['mother'])\n",
    "height_df['father_sqr'] = scale(height_df['father_sqr'])\n",
    "height_df['mother_sqr'] = scale(height_df['mother_sqr'])\n",
    "\n",
    "print(height_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the OLS with the categorical field 'gender':\n",
    "ols_model = sm.ols(formula = 'childHeight ~ father + mother + father_sqr + mother_sqr + gender + 1', data=height_df)\n",
    "\n",
    "results = ols_model.fit()\n",
    "n_points = height_df.shape[0]\n",
    "y_output = height_df['childHeight'].reshape(n_points, 1)\n",
    "\n",
    "# Get slope (m) and y-intercept (b)\n",
    "print('Intercept, Slopes : \\n{}'.format(results.params))\n",
    "\n",
    "# Get the t-values (hypothesis test statistics) for linear regression coefficient hypothesis tests.\n",
    "print('Intercept t-value, Slope t-values: \\n{}'.format(results.tvalues))\n",
    "\n",
    "# Get p-values for above t-value statistics\n",
    "print('\\nHypothesis test summary for each coefficient if they differ from zero:')\n",
    "print(results.pvalues)\n",
    "\n",
    "print('\\nSSE, SST, SSR, and RMSE:')\n",
    "mean_y = np.mean(y_output)\n",
    "sst = np.sum((y_output - mean_y)**2)\n",
    "sse = sst - results.ssr\n",
    "print('SSE: {}'.format(sse))\n",
    "print('SST: {}'.format(sst))\n",
    "print('SSR: {}'.format(results.ssr))\n",
    "print('RMSE: {}'.format(np.sqrt(results.mse_model)))\n",
    "\n",
    "# Get most of the linear regression statistics we are interested in:\n",
    "print(results.summary())\n",
    "\n",
    "# Plot a histogram of the residuals\n",
    "sns.distplot(results.resid, hist=True)\n",
    "plt.xlabel('Residual')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Residual Histogram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------\n",
    "## Total Regression\n",
    "### Also known as Demming Regression or Orthagonal Distance Regression\n",
    "------------------\n",
    "\n",
    "Another case of regression we may be interested in is minimizing the total error.  By total error, we mean minimizing the error in the y-values _and_ the x-values.  Here, we will be interested in minimizing the distance between the point and the best fit line for all points.\n",
    "\n",
    "A visual explanation of the distance we wish to minimize is in the following figure.  Regular regression minimizes the grey-vertical lines.  Total Regression will minimize the dashed red lines.\n",
    "\n",
    "<img src=\"https://library.startlearninglabs.uw.edu/DATASCI410/img/total_vs_leastsquares.png\" alt=\"Total Regression Errors\" style=\"width: 400px;\"/>\n",
    "\n",
    "### When would you use Total Regression?\n",
    "\n",
    " - When you have uncertainty in both the y-values _and_ the x-values.\n",
    " - When both the x and y values are the same scales and units.\n",
    "   - Note that it is recommended to scale both the y and x before total regression.\n",
    "   - There are alternative methods here that will scale the distance metric to be proportional to both the x and y measurement scales. However, such methods are not covered in this class and left to the reader.  For more detail on how to accomplish this, read the documentation in the scipy docs here: https://docs.scipy.org/doc/scipy-0.18.1/reference/odr.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description of method:\n",
    "#    https://docs.scipy.org/doc/scipy-0.18.1/reference/odr.html\n",
    "\n",
    "from scipy.odr import Model, Data, ODR\n",
    "from scipy.stats import linregress\n",
    "import numpy as np\n",
    "\n",
    "# Create test data\n",
    "x = np.linspace(start=0, stop=20., num=25)\n",
    "y = x + np.random.normal(size=25, scale=5)\n",
    "my_data = Data(x, y)\n",
    "my_data_df = pd.DataFrame({'x': x, 'y': y})\n",
    "\n",
    "# Define a linear function.\n",
    "def f(B, x):\n",
    "    '''Linear function y = m*x + b'''\n",
    "    # B is a vector of the parameters.\n",
    "    # x is an array of the current x values.\n",
    "    return B[0]*x + B[1]\n",
    "\n",
    "# Instantiate the linear model\n",
    "linear = Model(f)\n",
    "\n",
    "# Create the model with initial parameter guesses\n",
    "total_regression_model = ODR(my_data, linear, beta0=[1., 1.])\n",
    "\n",
    "# Fit the model\n",
    "total_reg_results = total_regression_model.run()\n",
    "\n",
    "# Print results\n",
    "total_reg_results.pprint()\n",
    "\n",
    "total_slope = total_reg_results.beta[0]\n",
    "total_intercept = total_reg_results.beta[1]\n",
    "print('\\nTotal Regression:')\n",
    "print('\\n  -Slope: {}'.format(total_slope))\n",
    "print('\\n  -Intercept: {}'.format(total_intercept))\n",
    "\n",
    "total_y_fit = [total_slope * xi + total_intercept for xi in x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the results graphically to regular regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_model = sm.ols(formula = 'y ~ x', data=my_data_df)\n",
    "\n",
    "results = ols_model.fit()\n",
    "n_points = my_data_df.shape[0]\n",
    "y_output = my_data_df['y'].values.reshape(n_points, 1)\n",
    "\n",
    "reg_slope = results.params[1]\n",
    "reg_intercept = results.params[0]\n",
    "print('\\nRegular Regression:')\n",
    "print('\\n  -Slope: {}'.format(reg_slope))\n",
    "print('\\n  -Intercept: {}'.format(reg_intercept))\n",
    "\n",
    "reg_y_fit = [reg_slope * xi + reg_intercept for xi in x]\n",
    "\n",
    "print('\\nSSE, SST, SSR, and RMSE:')\n",
    "mean_y = np.mean(y_output)\n",
    "sst = np.sum((y_output - mean_y)**2)\n",
    "sse = sst - results.ssr\n",
    "print('SSE: {}'.format(sse))\n",
    "print('SST: {}'.format(sst))\n",
    "print('SSR: {}'.format(results.ssr))\n",
    "print('RMSE: {}'.format(np.sqrt(results.mse_model)))\n",
    "\n",
    "# Get most of the linear regression statistics we are interested in:\n",
    "# print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import collections  as mc\n",
    "import pylab as pl\n",
    "\n",
    "# More terse equations:\n",
    "m = total_slope\n",
    "b = total_intercept\n",
    "\n",
    "# Loop through and draw vertical + total regression errors\n",
    "total_errs = []\n",
    "reg_errs = []\n",
    "for xi, yi, totaly, regy in zip(x, y, total_y_fit, reg_y_fit):\n",
    "    # Total reg. error segment\n",
    "    x_t = (yi + xi/m - b) / (m + 1/m)\n",
    "    y_t = m * x_t + b\n",
    "    temp_total_err = [(x_t, y_t), (xi, yi)]\n",
    "    total_errs.append(temp_total_err)\n",
    "    # Regular reg. error segment\n",
    "    temp_reg_err = [(xi, regy), (xi, yi)]\n",
    "    reg_errs.append(temp_reg_err)\n",
    "\n",
    "reg_lc = mc.LineCollection(reg_errs, colors='blue')\n",
    "total_lc = mc.LineCollection(total_errs, colors='red')\n",
    "fig, ax = pl.subplots(figsize=(6, 10))\n",
    "ax.add_collection(reg_lc)\n",
    "ax.add_collection(total_lc)\n",
    "\n",
    "plt.plot(x, y, 'o')\n",
    "plt.plot(x, reg_y_fit, color='blue', label='Regular')\n",
    "plt.plot(x, total_y_fit, color='red', label='Total')\n",
    "\n",
    "plt.title('Regular vs Total Linear Regression')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's compare the SSR's:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssr_reg = results.ssr\n",
    "ssr_total = np.sum([(yi - y_hat)**2 for yi, y_hat in zip(y, total_y_fit)])\n",
    "\n",
    "print('Regular SSE: {}'.format(ssr_reg))\n",
    "print('Total SSE: {}'.format(ssr_total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is as expected, because both SSR computations here based off the vertical distance.  Let's compute a new SSR for the total regression that is the sum-squared of the total-errors.\n",
    "\n",
    "We have prior calculated all pairs of points, in the variable, `total_errs`.  Let's use that to compute a new SSR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squared_errs = []\n",
    "for pair in total_errs:\n",
    "    temp_squared_dist = (pair[0][1] - pair[1][1])**2 + (pair[0][0] - pair[1][0])**2\n",
    "    squared_errs.append(temp_squared_dist)\n",
    "    \n",
    "new_total_ssr = np.sum(squared_errs)\n",
    "print('New Total SSE: {}'.format(new_total_ssr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note, however, it is harder to define the SST here, and as a result, any definition of an $R^{2}$ metric is arbitrary and not-comparable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Until now, we have been working strictly with linear regression models. Now we will look at a widely used variation on the linear model know as **logistic regression**.\n",
    "\n",
    "Logistic regression is widely used as a classification model. Logistic regression is linear model, with a binary response, `{False, True}` or `{0, 1}`.  However, the response is computed as a log likelihood. In the simplest case, the response has a Binomial distribution. \n",
    "\n",
    "The response of the linear model is transformed to the log likelihood using a sigmodial function, also know as the **logistic function**:\n",
    "\n",
    "$$f(x) = \\frac{1}{1 + e^{-\\kappa(x - x_0)}} \\\\\n",
    "\\kappa = steepness$$\n",
    "\n",
    "Execute the code in the cell below to compute and plot an example of the logistic function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the logistic transformation function (f(x) above)\n",
    "x_seq = np.linspace(-7, 7, 100)\n",
    "\n",
    "def log_fun(x, center=0, scale=1):\n",
    "    e = np.exp(-scale*(x-center))\n",
    "    log_out = 1./(1. + e)\n",
    "    return log_out\n",
    "\n",
    "log_fun_vectorized = np.vectorize(log_fun)\n",
    "\n",
    "log_y = log_fun_vectorized(x_seq)\n",
    "\n",
    "plt.plot(x_seq, log_y)\n",
    "plt.title('Standard Logistic Function')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make this a bit more concrete with a simple example. Say we have a linear model:\n",
    "\n",
    "$$\\hat{y} = \\beta_0 + \\beta_1\\ x$$\n",
    "\n",
    "Now, depending on the value of $\\hat{y}$ we want to classify the output from a logistic regression model as either `0` or `1`. We can use the linear model in the logistic function as follows:\n",
    "\n",
    "$$F(\\hat{y}) = \\frac{1}{1 + e^{-\\kappa(\\beta_0 + \\beta_1\\ x)}} $$\n",
    "\n",
    "In this way we transform the continious output of the linear model defined on $-\\infty \\le \\hat{y} \\le \\infty$ to a binary response, $0 \\le F(\\hat{y}) \\le 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Example\n",
    "\n",
    "Next, we will try to classify the gender of the children in the Galton families data set using logistic regression on the height data.\n",
    "\n",
    "As a first step, we need to create a scaled model matrix of the features for the logistic regression. Run the code in the cell bellow to compute this matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The logit function in Stats models does not take formulas,\n",
    "#     instead we have to give it arrays/matrices.\n",
    "#     This is very common with ML in python, so it is a good\n",
    "#     time to get familiar with the format.\n",
    "\n",
    "# Create intercept column\n",
    "height_df['intercept'] = 1.0\n",
    "\n",
    "X_cols = ['father', 'mother', 'childHeight', 'intercept']\n",
    "\n",
    "X = height_df[X_cols]\n",
    "Y = np.array([1 if x=='F' else 0 for x in height_df['gender']])\n",
    "\n",
    "logit_model = sm.Logit(Y, X)\n",
    "\n",
    "result = logit_model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix pending for issue in statsmodels:\n",
    "from scipy import stats\n",
    "stats.chisqprob = lambda chisq, df: stats.chi2.sf(chisq, df)\n",
    "\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can generate a `alpha` level of confidence around the parameter estimates with the result method `results.conf_int(alpha=...)` as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confidence intervals\n",
    "# alpha is the % confidence in parameters.\n",
    "print(result.conf_int(alpha=0.90))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation of parameters\n",
    "\n",
    "If we are interested in how the output probabilities change with respect to the features, we can look at the odds ratio. The odds ratio is just the exponential of the parameters.\n",
    "\n",
    "These tell us how a 1 unit increase or decrease in a variable affects the odds of being predicting a '1' (Female). For example, we can expect the odds of predicting a female gender to decrease by about 65.9% if we find the childHeight goes down by 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Odds ratio\n",
    "print(np.exp(result.params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how we can make predictions with our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardized Father, Standardized Mother, ChildHeight, 1.0 (intercept)\n",
    "test_input = np.array([3.7, 1.26, 70.1, 1.0])\n",
    "test_output = result.predict(test_input)\n",
    "print('Probability of Female: {0:.2f}%'.format(test_output[0]*100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding out the effect of `childHeight` on the probabilities graphically:\n",
    "\n",
    "Here we will loop over a sequence of childHeights to see the effect of them on the probability of a Female gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "childHeight_seq = np.linspace(height_df['childHeight'].min(), height_df['childHeight'].max(), 100)\n",
    "\n",
    "avg_father = height_df['father'].mean()\n",
    "avg_mother = height_df['mother'].mean()\n",
    "\n",
    "childHeight_df = pd.DataFrame(childHeight_seq, columns=['childHeight'])\n",
    "childHeight_df['father'] = height_df['father'].mean() # hopefully close to zero\n",
    "childHeight_df['mother'] = height_df['mother'].mean() # hopefully close to zero\n",
    "childHeight_df['intercept'] = 1.0\n",
    "\n",
    "# Rearrange order of columns appropriately\n",
    "childHeight_df = childHeight_df[['father', 'mother', 'childHeight', 'intercept']]\n",
    "\n",
    "# Get probability predictions\n",
    "gender_prob_childHeight = result.predict(childHeight_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(childHeight_seq, gender_prob_childHeight)\n",
    "plt.title('P(F) vs childHeight')\n",
    "plt.xlabel('childHeight')\n",
    "plt.ylabel('P(F)')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are also interested in evaluating our classifier's accuracy.  Here we will look at how many are:\n",
    "\n",
    " - True Positives (TP): We predicted Female and the actual gender was Female.\n",
    " - True Negatives (TN): We predicted Male and the actual gender was Male.\n",
    " - False Positives (FP): We predicted Female and the actual gender was Male.\n",
    " - False Negatives (FN): We predicted Male and the actual gender was Female.\n",
    "\n",
    "The output of our predictions is a probability.  In order to make this a binary prediction (Female or male), we need to decide a cutoff.  Commonly, we choose 0.5 as a cutoff.  But know that this choice is arbitrary and we can set it to whatever probability we choose.  If we have a model where False Positives are costly, we might decide to increase the cutoff, and vice-versa if False Negatives are more costly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_frame = X.copy()\n",
    "\n",
    "prediction_frame['probability'] = result.predict(X)\n",
    "\n",
    "prediction_frame['actual'] = Y\n",
    "\n",
    "cutoff = 0.5\n",
    "def prediction_fun(row):\n",
    "    if row['probability'] > 0.5:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return 0.0\n",
    "    \n",
    "\n",
    "prediction_frame['prediction'] = prediction_frame.apply(prediction_fun, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_frame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Now let's calculate the accuracy, true positives, true negatives, false positives, and false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = np.sum([a == 1.0 and p == 1.0 for a, p in zip(prediction_frame['actual'], prediction_frame['prediction'])])\n",
    "TN = np.sum([a == 0.0 and p == 0.0 for a, p in zip(prediction_frame['actual'], prediction_frame['prediction'])])\n",
    "\n",
    "FP = np.sum([a == 0.0 and p == 1.0 for a, p in zip(prediction_frame['actual'], prediction_frame['prediction'])])\n",
    "FN = np.sum([a == 1.0 and p == 0.0 for a, p in zip(prediction_frame['actual'], prediction_frame['prediction'])])\n",
    "\n",
    "num_obs = prediction_frame.shape[0]\n",
    "\n",
    "print('Out of {} observations:'.format(num_obs))\n",
    "print('TP: {}, TN: {}'.format(TP, TN))\n",
    "print('\\nFP: {}, FN: {}'.format(FP, FN))\n",
    "\n",
    "accuracy = (TP + TN) / num_obs\n",
    "print('\\nAccuracy over two classes: {0:.2f}%'.format(accuracy*100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "We can view these results in a table with a confusion matrix as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Actual = columns')\n",
    "print('\\nPredicted = rows')\n",
    "print('\\n   0.0   1.0')\n",
    "print('----------------')\n",
    "print(result.pred_table(threshold=cutoff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the confusion matrix, males are defined as Positive cases and females are Negative cases. Notice that most of the cases in this data are correctly classified with only a few false negatives and false positives.\n",
    "\n",
    "The other metrics are defined as follows:\n",
    "\n",
    "- Accuracy = (TP + TN) / (TP + FP + TN + FN)\n",
    "- Precision = TP / (TP + FP)\n",
    "   - Precision is the fraction of the relevant class predictions are actually correct.\n",
    "- Recall = TP / (TP + FN)\n",
    "   - Recall is the fraction of the relevant class were we able to predict.\n",
    "\n",
    "These summary statistics show the classifier works fairly well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Summary\n",
    "In this notebook, we looked at 3 methods as a solution to the problem of overfit models:\n",
    "\n",
    "* Stepwise Regression to eliminate features one at a time\n",
    "* Singular Value Decomposition to find meaningful features\n",
    "* Lasso, Ridge, and Elastic-net Regularization to stabilize over-parameterized models.\n",
    "\n",
    "Important terms from this notebook include:\n",
    "\n",
    "* AIC - the model log-likelihood adjusted for the number of model parameters\n",
    "* Deviance - a measure of the relative likelihood of the model (generalization of variance)\n",
    "* Conformable - number of rows of first matrix equals the number of columns of the second matrix, number of columns of first matrix equals number of rows of the second matrix\n",
    "* SVD - Singular Value Decomposition describes a matrix as linear combination of a series of vectors $U$, $V$, $D$\n",
    "* singular values $s$ - the diagonal matrix of singular values used as a scaling term\n",
    "* Rank deficient matrix - has one or more of the $m$ singular values  $d_i  \\sim 0.0$  \n",
    "* Lambda - a small bias term added to singular values to stabilize the inverse singular value matrix\n",
    "* PCA - Principal Component Analysis to determine what new features can be made from linear combinations that are independent of each other by using functions and the SVD\n",
    "* PCR - Principal Component Regression uses the PCA to perform a \"regularized\" regression.\n",
    "* L2 Norm regularization - Ridge regularization limiting the Euclidean norm of values of the model coefficient vector\n",
    "* L1 regularization - Lasso regularization using the Manhattan norm\n",
    "* Elastic-net algorithm - a weighted combination of L2 and L1 regularization\n",
    "* Total regression - minimizes the distance between the point and the best fit line for all points\n",
    "* Logistic regression - a linear model, with a binary response (Binomial distribution) using the logistic function on the response to transform it into a log likelihood\n",
    "\n",
    "\n",
    "And we reviewed Linear Algebra operations:\n",
    "* adding vectors and matrices\n",
    "* multiplying vectors and matrices\n",
    "* transposing matrices with `np.transpose`\n",
    "* dot product or scalar product or inner product with `np.dot`\n",
    "* L2 norm of a vector\n",
    "* identity matrix with `np.eye`\n",
    "* inverse of a matrix with `np.linalg.inv`\n",
    "* singular value decomposition with `np.linalg.svd`\n",
    "* creating a diagonal matrix with `np.diag`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"reminder\" style=\"border-radius: 5px; background-color:#f5f5f5;\" >\n",
    "<h3>Reminder</h3>\n",
    "<p>Use this notebook to answer the quiz questions related to the <b>Your Turn</b> sections.<p>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
